import os
import sys
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ê³ ë„í™”ëœ feature_extractor.py ë¶ˆëŸ¬ì˜¤ê¸°
from feature_extractor import AudioFeatureExtractor

# 1. ê²½ë¡œ ë° ì„¤ì •
BASE_PATH = '/home/ubuntu/voice_analysis/data/final_data'
CATEGORIES = [
    'ë°°ê²½ ì†ŒìŒ í´ë˜ìŠ¤', 
    'ìŒì„± í´ë˜ìŠ¤', 
    'Processed_Data_Full/Scream'
]

def save_confusion_matrix(y_test, y_pred, classes, model_name):
    """í•™ìŠµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.title(f'Confusion Matrix - {model_name} (High-Dim Features)', fontsize=15)
    plt.ylabel('Actual Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(f'result_chart_{model_name}_v2.png')
    print(f"-> ì„±ëŠ¥ ì°¨íŠ¸ê°€ 'result_chart_{model_name}_v2.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()

def main():
    print("="*50)
    print("ê³ ë„í™”ëœ ìŒì„± ë¶„ì„ ëª¨ë¸ ì¬í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("íŠ¹ì§•ê°’: Mel + Delta + Centroid + ZCR (514ì°¨ì›)")
    print("="*50)

    # 2. íŠ¹ì§• ì¶”ì¶œ (ì „ì²˜ë¦¬)
    extractor = AudioFeatureExtractor(duration=3)
    
    # ê° í´ë˜ìŠ¤ë‹¹ 10,000ê°œ ì¶”ì¶œ (ê³ ë„í™”ëœ extract_mel_features ì‚¬ìš©)
    X, y = extractor.build_dataset(BASE_PATH, CATEGORIES, max_files_per_class=10000)
    
    if len(X) == 0:
        print("ì—ëŸ¬: ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # [Hard Negative Mining] ê¸°ê³„ ì†ŒìŒ ë°ì´í„° ë³´ê°•
    MACHINE_PATH = os.path.join(BASE_PATH, 'ë°°ê²½ ì†ŒìŒ í´ë˜ìŠ¤')
    MACHINE_SUB_DIR = ['Machine_Tool_Noise(ì¼ë°˜ ê¸°ê³„ë“¤)']
    
    print("\n[ì „ëµ] ê¸°ê³„ ì†ŒìŒ ì˜¤íƒ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ê°€ ë°ì´í„° ë³´ê°• ì¤‘...")
    try:
        X_extra, y_extra = extractor.build_dataset(MACHINE_PATH, MACHINE_SUB_DIR, max_files_per_class=5000)
        y_extra = np.array(['ë°°ê²½ ì†ŒìŒ í´ë˜ìŠ¤'] * len(y_extra))
        X = np.vstack([X, X_extra])
        y = np.hstack([y, y_extra])
        print(f"-> ê¸°ê³„ ì†ŒìŒ ë°ì´í„° {len(X_extra)}ê°œ ë³´ê°• ì™„ë£Œ. (ì´ ë°ì´í„°: {len(X)}ê°œ)")
    except Exception as e:
        print(f"ê¸°ê³„ ì†ŒìŒ ë³´ê°• ê±´ë„ˆëœ€: {e}")

    # 3. ë°ì´í„° ì¸ì½”ë”© ë° ë¶„í• 
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”: íŠ¹ì§•ë§ˆë‹¤ ë‹¨ìœ„ê°€ ë‹¤ë¥´ë¯€ë¡œ ë°˜ë“œì‹œ í•„ìš”)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 5. ëª¨ë¸ ì •ì˜ (ê³ ì°¨ì› ë°ì´í„°ì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
    models = {
        "XGBoost": XGBClassifier(
            n_estimators=500,       # íŠ¹ì§•ì´ ë§ì•„ì¡Œìœ¼ë¯€ë¡œ ë‚˜ë¬´ ê°œìˆ˜ ì¦ê°€
            learning_rate=0.05,     # ë” ì´˜ì´˜í•˜ê²Œ í•™ìŠµ
            max_depth=8,            # ë³µì¡í•œ ì†ŒìŒ íŒ¨í„´ì„ ì¡ê¸° ìœ„í•´ ê¹Šì´ ì¡°ì ˆ
            subsample=0.8,          # ê³¼ì í•© ë°©ì§€
            colsample_bytree=0.8,   # íŠ¹ì§• ì„ íƒ ë‹¤ì–‘í™”
            eval_metric='mlogloss',
            random_state=42,
            n_jobs=-1
        ),
        "RandomForest": RandomForestClassifier(
            n_estimators=500, 
            max_depth=20, 
            random_state=42, 
            n_jobs=-1
        )
    }

    # 6. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    results = {}
    for name, model in models.items():
        print(f"\n[{name}] ê³ ë„í™” í•™ìŠµ ì‹œì‘ (ì´ ê³¼ì •ì€ ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        acc = accuracy_score(y_test, y_pred)
        results[name] = acc
        
        print(f"{name} ìµœì¢… ì •í™•ë„: {acc:.4f}")
        print(classification_report(y_test, y_pred, target_names=le.classes_))
        
        # ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
        save_confusion_matrix(y_test, y_pred, le.classes_, name)

    # 7. ìµœì ì˜ ëª¨ë¸ ì €ì¥
    best_model_name = max(results, key=results.get)
    joblib.dump(models[best_model_name], 'best_voice_model_xgb.pkl')
    joblib.dump(le, 'label_encoder.pkl')
    joblib.dump(scaler, 'scaler.pkl')
    
    print("\n" + "="*50)
    print(f"ğŸ‰ ì¬í•™ìŠµ ì™„ë£Œ! ìµœì  ëª¨ë¸ ì €ì¥ë¨: {best_model_name}")
    print(f"ì „ì²´ í‰ê·  ì •í™•ë„: {results[best_model_name]:.4f}")
    print("="*50)

if __name__ == "__main__":
    main()